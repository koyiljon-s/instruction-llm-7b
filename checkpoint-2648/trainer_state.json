{
  "best_global_step": null,
  "best_metric": null,
  "best_model_checkpoint": null,
  "epoch": 2.0,
  "eval_steps": 500,
  "global_step": 2648,
  "is_hyper_param_search": false,
  "is_local_process_zero": true,
  "is_world_process_zero": true,
  "log_history": [
    {
      "epoch": 0.015108593012275733,
      "grad_norm": 0.638580322265625,
      "learning_rate": 0.000199043303121853,
      "loss": 2.1554,
      "step": 20
    },
    {
      "epoch": 0.030217186024551465,
      "grad_norm": 0.6124690175056458,
      "learning_rate": 0.00019803625377643507,
      "loss": 1.867,
      "step": 40
    },
    {
      "epoch": 0.0453257790368272,
      "grad_norm": 0.5913940668106079,
      "learning_rate": 0.00019702920443101713,
      "loss": 1.7297,
      "step": 60
    },
    {
      "epoch": 0.06043437204910293,
      "grad_norm": 0.7025613188743591,
      "learning_rate": 0.0001960221550855992,
      "loss": 1.7221,
      "step": 80
    },
    {
      "epoch": 0.07554296506137866,
      "grad_norm": 0.7115285992622375,
      "learning_rate": 0.00019501510574018128,
      "loss": 1.6675,
      "step": 100
    },
    {
      "epoch": 0.0906515580736544,
      "grad_norm": 0.6836743354797363,
      "learning_rate": 0.00019400805639476336,
      "loss": 1.6591,
      "step": 120
    },
    {
      "epoch": 0.10576015108593012,
      "grad_norm": 0.7489951252937317,
      "learning_rate": 0.00019300100704934542,
      "loss": 1.5474,
      "step": 140
    },
    {
      "epoch": 0.12086874409820586,
      "grad_norm": 0.6644171476364136,
      "learning_rate": 0.0001919939577039275,
      "loss": 1.5238,
      "step": 160
    },
    {
      "epoch": 0.1359773371104816,
      "grad_norm": 0.6642056703567505,
      "learning_rate": 0.0001909869083585096,
      "loss": 1.5068,
      "step": 180
    },
    {
      "epoch": 0.1510859301227573,
      "grad_norm": 0.6416130065917969,
      "learning_rate": 0.00018997985901309165,
      "loss": 1.4648,
      "step": 200
    },
    {
      "epoch": 0.16619452313503305,
      "grad_norm": 0.6407029032707214,
      "learning_rate": 0.00018897280966767373,
      "loss": 1.4511,
      "step": 220
    },
    {
      "epoch": 0.1813031161473088,
      "grad_norm": 0.6885688900947571,
      "learning_rate": 0.0001879657603222558,
      "loss": 1.4635,
      "step": 240
    },
    {
      "epoch": 0.1964117091595845,
      "grad_norm": 0.6579564213752747,
      "learning_rate": 0.00018695871097683788,
      "loss": 1.3833,
      "step": 260
    },
    {
      "epoch": 0.21152030217186024,
      "grad_norm": 0.6821775436401367,
      "learning_rate": 0.00018595166163141996,
      "loss": 1.3889,
      "step": 280
    },
    {
      "epoch": 0.22662889518413598,
      "grad_norm": 0.6145583987236023,
      "learning_rate": 0.00018494461228600202,
      "loss": 1.3926,
      "step": 300
    },
    {
      "epoch": 0.24173748819641172,
      "grad_norm": 0.6227560043334961,
      "learning_rate": 0.0001839375629405841,
      "loss": 1.3652,
      "step": 320
    },
    {
      "epoch": 0.25684608120868746,
      "grad_norm": 0.7036808133125305,
      "learning_rate": 0.00018293051359516616,
      "loss": 1.3351,
      "step": 340
    },
    {
      "epoch": 0.2719546742209632,
      "grad_norm": 0.6674836874008179,
      "learning_rate": 0.00018192346424974825,
      "loss": 1.2841,
      "step": 360
    },
    {
      "epoch": 0.2870632672332389,
      "grad_norm": 0.7363541722297668,
      "learning_rate": 0.0001809164149043303,
      "loss": 1.3102,
      "step": 380
    },
    {
      "epoch": 0.3021718602455146,
      "grad_norm": 0.6525669097900391,
      "learning_rate": 0.0001799093655589124,
      "loss": 1.2601,
      "step": 400
    },
    {
      "epoch": 0.31728045325779036,
      "grad_norm": 0.6826169490814209,
      "learning_rate": 0.00017890231621349448,
      "loss": 1.2788,
      "step": 420
    },
    {
      "epoch": 0.3323890462700661,
      "grad_norm": 0.6846610307693481,
      "learning_rate": 0.00017789526686807654,
      "loss": 1.2413,
      "step": 440
    },
    {
      "epoch": 0.34749763928234184,
      "grad_norm": 0.737987756729126,
      "learning_rate": 0.00017688821752265862,
      "loss": 1.2721,
      "step": 460
    },
    {
      "epoch": 0.3626062322946176,
      "grad_norm": 0.6671044826507568,
      "learning_rate": 0.0001758811681772407,
      "loss": 1.2122,
      "step": 480
    },
    {
      "epoch": 0.3777148253068933,
      "grad_norm": 0.62265545129776,
      "learning_rate": 0.00017487411883182277,
      "loss": 1.2113,
      "step": 500
    },
    {
      "epoch": 0.392823418319169,
      "grad_norm": 0.7344897389411926,
      "learning_rate": 0.00017386706948640482,
      "loss": 1.1999,
      "step": 520
    },
    {
      "epoch": 0.40793201133144474,
      "grad_norm": 0.6948499083518982,
      "learning_rate": 0.0001728600201409869,
      "loss": 1.2143,
      "step": 540
    },
    {
      "epoch": 0.4230406043437205,
      "grad_norm": 0.5802909135818481,
      "learning_rate": 0.000171852970795569,
      "loss": 1.1561,
      "step": 560
    },
    {
      "epoch": 0.4381491973559962,
      "grad_norm": 0.6689253449440002,
      "learning_rate": 0.00017084592145015105,
      "loss": 1.1617,
      "step": 580
    },
    {
      "epoch": 0.45325779036827196,
      "grad_norm": 0.666337788105011,
      "learning_rate": 0.00016983887210473314,
      "loss": 1.1379,
      "step": 600
    },
    {
      "epoch": 0.4683663833805477,
      "grad_norm": 0.708098828792572,
      "learning_rate": 0.00016883182275931522,
      "loss": 1.1678,
      "step": 620
    },
    {
      "epoch": 0.48347497639282344,
      "grad_norm": 0.6483791470527649,
      "learning_rate": 0.00016782477341389728,
      "loss": 1.1626,
      "step": 640
    },
    {
      "epoch": 0.4985835694050991,
      "grad_norm": 0.7273135185241699,
      "learning_rate": 0.00016681772406847934,
      "loss": 1.1553,
      "step": 660
    },
    {
      "epoch": 0.5136921624173749,
      "grad_norm": 0.7223193049430847,
      "learning_rate": 0.00016581067472306145,
      "loss": 1.1786,
      "step": 680
    },
    {
      "epoch": 0.5288007554296507,
      "grad_norm": 0.8509509563446045,
      "learning_rate": 0.0001648036253776435,
      "loss": 1.1515,
      "step": 700
    },
    {
      "epoch": 0.5439093484419264,
      "grad_norm": 0.6471524238586426,
      "learning_rate": 0.00016379657603222557,
      "loss": 1.1366,
      "step": 720
    },
    {
      "epoch": 0.559017941454202,
      "grad_norm": 0.6572981476783752,
      "learning_rate": 0.00016278952668680768,
      "loss": 1.1219,
      "step": 740
    },
    {
      "epoch": 0.5741265344664778,
      "grad_norm": 0.6402274370193481,
      "learning_rate": 0.00016178247734138974,
      "loss": 1.1575,
      "step": 760
    },
    {
      "epoch": 0.5892351274787535,
      "grad_norm": 0.6382126212120056,
      "learning_rate": 0.0001607754279959718,
      "loss": 1.1673,
      "step": 780
    },
    {
      "epoch": 0.6043437204910292,
      "grad_norm": 0.6923805475234985,
      "learning_rate": 0.00015976837865055388,
      "loss": 1.1344,
      "step": 800
    },
    {
      "epoch": 0.619452313503305,
      "grad_norm": 0.6478564143180847,
      "learning_rate": 0.00015876132930513597,
      "loss": 1.1009,
      "step": 820
    },
    {
      "epoch": 0.6345609065155807,
      "grad_norm": 0.6106889247894287,
      "learning_rate": 0.00015775427995971803,
      "loss": 1.1523,
      "step": 840
    },
    {
      "epoch": 0.6496694995278565,
      "grad_norm": 0.6930863857269287,
      "learning_rate": 0.0001567472306143001,
      "loss": 1.1116,
      "step": 860
    },
    {
      "epoch": 0.6647780925401322,
      "grad_norm": 0.6119394898414612,
      "learning_rate": 0.0001557401812688822,
      "loss": 1.1025,
      "step": 880
    },
    {
      "epoch": 0.6798866855524079,
      "grad_norm": 0.7265839576721191,
      "learning_rate": 0.00015473313192346426,
      "loss": 1.1264,
      "step": 900
    },
    {
      "epoch": 0.6949952785646837,
      "grad_norm": 0.7039998173713684,
      "learning_rate": 0.00015372608257804631,
      "loss": 1.099,
      "step": 920
    },
    {
      "epoch": 0.7101038715769594,
      "grad_norm": 0.7395510077476501,
      "learning_rate": 0.00015271903323262843,
      "loss": 1.0824,
      "step": 940
    },
    {
      "epoch": 0.7252124645892352,
      "grad_norm": 0.6820393204689026,
      "learning_rate": 0.00015171198388721049,
      "loss": 1.0608,
      "step": 960
    },
    {
      "epoch": 0.7403210576015109,
      "grad_norm": 0.7570927143096924,
      "learning_rate": 0.00015070493454179254,
      "loss": 1.1041,
      "step": 980
    },
    {
      "epoch": 0.7554296506137866,
      "grad_norm": 0.5917067527770996,
      "learning_rate": 0.00014969788519637463,
      "loss": 1.1095,
      "step": 1000
    },
    {
      "epoch": 0.7705382436260623,
      "grad_norm": 0.7482699751853943,
      "learning_rate": 0.00014869083585095671,
      "loss": 1.1072,
      "step": 1020
    },
    {
      "epoch": 0.785646836638338,
      "grad_norm": 0.8510087132453918,
      "learning_rate": 0.00014768378650553877,
      "loss": 1.0509,
      "step": 1040
    },
    {
      "epoch": 0.8007554296506137,
      "grad_norm": 0.6339656710624695,
      "learning_rate": 0.00014667673716012086,
      "loss": 1.0813,
      "step": 1060
    },
    {
      "epoch": 0.8158640226628895,
      "grad_norm": 0.7859771847724915,
      "learning_rate": 0.00014566968781470294,
      "loss": 1.0397,
      "step": 1080
    },
    {
      "epoch": 0.8309726156751652,
      "grad_norm": 0.6934078931808472,
      "learning_rate": 0.000144662638469285,
      "loss": 1.0283,
      "step": 1100
    },
    {
      "epoch": 0.846081208687441,
      "grad_norm": 0.8056946396827698,
      "learning_rate": 0.0001436555891238671,
      "loss": 1.0491,
      "step": 1120
    },
    {
      "epoch": 0.8611898016997167,
      "grad_norm": 0.6803275942802429,
      "learning_rate": 0.00014264853977844914,
      "loss": 1.068,
      "step": 1140
    },
    {
      "epoch": 0.8762983947119924,
      "grad_norm": 0.6918123364448547,
      "learning_rate": 0.00014164149043303123,
      "loss": 1.078,
      "step": 1160
    },
    {
      "epoch": 0.8914069877242682,
      "grad_norm": 0.7286608219146729,
      "learning_rate": 0.0001406344410876133,
      "loss": 1.0838,
      "step": 1180
    },
    {
      "epoch": 0.9065155807365439,
      "grad_norm": 0.7803124785423279,
      "learning_rate": 0.00013962739174219537,
      "loss": 1.0333,
      "step": 1200
    },
    {
      "epoch": 0.9216241737488197,
      "grad_norm": 0.7093648314476013,
      "learning_rate": 0.00013862034239677746,
      "loss": 1.0212,
      "step": 1220
    },
    {
      "epoch": 0.9367327667610954,
      "grad_norm": 0.7172715067863464,
      "learning_rate": 0.00013761329305135952,
      "loss": 1.0225,
      "step": 1240
    },
    {
      "epoch": 0.9518413597733711,
      "grad_norm": 0.8255679607391357,
      "learning_rate": 0.0001366062437059416,
      "loss": 1.0294,
      "step": 1260
    },
    {
      "epoch": 0.9669499527856469,
      "grad_norm": 0.6921229362487793,
      "learning_rate": 0.00013559919436052366,
      "loss": 1.0184,
      "step": 1280
    },
    {
      "epoch": 0.9820585457979226,
      "grad_norm": 0.7010710835456848,
      "learning_rate": 0.00013459214501510575,
      "loss": 1.0473,
      "step": 1300
    },
    {
      "epoch": 0.9971671388101983,
      "grad_norm": 0.6477928161621094,
      "learning_rate": 0.00013358509566968783,
      "loss": 1.0388,
      "step": 1320
    },
    {
      "epoch": 1.0120868744098206,
      "grad_norm": 0.6787996888160706,
      "learning_rate": 0.0001325780463242699,
      "loss": 0.958,
      "step": 1340
    },
    {
      "epoch": 1.0271954674220962,
      "grad_norm": 0.6528811454772949,
      "learning_rate": 0.00013157099697885198,
      "loss": 0.9142,
      "step": 1360
    },
    {
      "epoch": 1.042304060434372,
      "grad_norm": 0.7415065765380859,
      "learning_rate": 0.00013056394763343403,
      "loss": 0.9262,
      "step": 1380
    },
    {
      "epoch": 1.0574126534466477,
      "grad_norm": 0.7211471199989319,
      "learning_rate": 0.00012955689828801612,
      "loss": 0.955,
      "step": 1400
    },
    {
      "epoch": 1.0725212464589235,
      "grad_norm": 0.7308480739593506,
      "learning_rate": 0.00012854984894259818,
      "loss": 0.9251,
      "step": 1420
    },
    {
      "epoch": 1.0876298394711992,
      "grad_norm": 0.7025598883628845,
      "learning_rate": 0.00012754279959718026,
      "loss": 0.9242,
      "step": 1440
    },
    {
      "epoch": 1.102738432483475,
      "grad_norm": 0.6775097846984863,
      "learning_rate": 0.00012653575025176235,
      "loss": 0.9489,
      "step": 1460
    },
    {
      "epoch": 1.1178470254957507,
      "grad_norm": 0.652665376663208,
      "learning_rate": 0.0001255287009063444,
      "loss": 0.8836,
      "step": 1480
    },
    {
      "epoch": 1.1329556185080265,
      "grad_norm": 0.6998504400253296,
      "learning_rate": 0.0001245216515609265,
      "loss": 0.9681,
      "step": 1500
    },
    {
      "epoch": 1.1480642115203021,
      "grad_norm": 0.6345914006233215,
      "learning_rate": 0.00012351460221550858,
      "loss": 0.9424,
      "step": 1520
    },
    {
      "epoch": 1.163172804532578,
      "grad_norm": 0.6555542945861816,
      "learning_rate": 0.00012250755287009064,
      "loss": 0.8946,
      "step": 1540
    },
    {
      "epoch": 1.1782813975448536,
      "grad_norm": 0.7820575833320618,
      "learning_rate": 0.00012150050352467271,
      "loss": 0.909,
      "step": 1560
    },
    {
      "epoch": 1.1933899905571295,
      "grad_norm": 0.6515637636184692,
      "learning_rate": 0.00012049345417925479,
      "loss": 0.9255,
      "step": 1580
    },
    {
      "epoch": 1.208498583569405,
      "grad_norm": 0.6812759637832642,
      "learning_rate": 0.00011948640483383686,
      "loss": 0.8935,
      "step": 1600
    },
    {
      "epoch": 1.2236071765816807,
      "grad_norm": 0.6611970067024231,
      "learning_rate": 0.00011847935548841894,
      "loss": 0.8981,
      "step": 1620
    },
    {
      "epoch": 1.2387157695939566,
      "grad_norm": 0.7894744277000427,
      "learning_rate": 0.00011747230614300102,
      "loss": 0.8934,
      "step": 1640
    },
    {
      "epoch": 1.2538243626062324,
      "grad_norm": 0.7084535360336304,
      "learning_rate": 0.0001164652567975831,
      "loss": 0.8972,
      "step": 1660
    },
    {
      "epoch": 1.268932955618508,
      "grad_norm": 0.6829096674919128,
      "learning_rate": 0.00011545820745216515,
      "loss": 0.9353,
      "step": 1680
    },
    {
      "epoch": 1.2840415486307837,
      "grad_norm": 0.6458596587181091,
      "learning_rate": 0.00011445115810674725,
      "loss": 0.8825,
      "step": 1700
    },
    {
      "epoch": 1.2991501416430595,
      "grad_norm": 0.716256856918335,
      "learning_rate": 0.00011344410876132931,
      "loss": 0.9416,
      "step": 1720
    },
    {
      "epoch": 1.3142587346553352,
      "grad_norm": 0.6466896533966064,
      "learning_rate": 0.00011243705941591138,
      "loss": 0.9052,
      "step": 1740
    },
    {
      "epoch": 1.329367327667611,
      "grad_norm": 0.7117520570755005,
      "learning_rate": 0.00011143001007049345,
      "loss": 0.9238,
      "step": 1760
    },
    {
      "epoch": 1.3444759206798866,
      "grad_norm": 0.6773107647895813,
      "learning_rate": 0.00011042296072507554,
      "loss": 0.8678,
      "step": 1780
    },
    {
      "epoch": 1.3595845136921625,
      "grad_norm": 0.7109878659248352,
      "learning_rate": 0.00010941591137965761,
      "loss": 0.9376,
      "step": 1800
    },
    {
      "epoch": 1.3746931067044381,
      "grad_norm": 0.6877878904342651,
      "learning_rate": 0.00010840886203423968,
      "loss": 0.8701,
      "step": 1820
    },
    {
      "epoch": 1.3898016997167137,
      "grad_norm": 0.7716924548149109,
      "learning_rate": 0.00010740181268882177,
      "loss": 0.8978,
      "step": 1840
    },
    {
      "epoch": 1.4049102927289896,
      "grad_norm": 0.7168180346488953,
      "learning_rate": 0.00010639476334340384,
      "loss": 0.8938,
      "step": 1860
    },
    {
      "epoch": 1.4200188857412654,
      "grad_norm": 0.7712883353233337,
      "learning_rate": 0.00010538771399798591,
      "loss": 0.9121,
      "step": 1880
    },
    {
      "epoch": 1.435127478753541,
      "grad_norm": 0.742200493812561,
      "learning_rate": 0.00010438066465256797,
      "loss": 0.9196,
      "step": 1900
    },
    {
      "epoch": 1.4502360717658167,
      "grad_norm": 0.6934712529182434,
      "learning_rate": 0.00010337361530715007,
      "loss": 0.9169,
      "step": 1920
    },
    {
      "epoch": 1.4653446647780926,
      "grad_norm": 0.7364057302474976,
      "learning_rate": 0.00010236656596173213,
      "loss": 0.8874,
      "step": 1940
    },
    {
      "epoch": 1.4804532577903684,
      "grad_norm": 0.7111160755157471,
      "learning_rate": 0.0001013595166163142,
      "loss": 0.8839,
      "step": 1960
    },
    {
      "epoch": 1.495561850802644,
      "grad_norm": 0.7938785552978516,
      "learning_rate": 0.00010035246727089628,
      "loss": 0.9016,
      "step": 1980
    },
    {
      "epoch": 1.5106704438149197,
      "grad_norm": 0.7610501050949097,
      "learning_rate": 9.934541792547835e-05,
      "loss": 0.8903,
      "step": 2000
    },
    {
      "epoch": 1.5257790368271955,
      "grad_norm": 0.6649030447006226,
      "learning_rate": 9.833836858006043e-05,
      "loss": 0.9041,
      "step": 2020
    },
    {
      "epoch": 1.5408876298394714,
      "grad_norm": 0.6983354687690735,
      "learning_rate": 9.73313192346425e-05,
      "loss": 0.8738,
      "step": 2040
    },
    {
      "epoch": 1.5559962228517468,
      "grad_norm": 0.7853218913078308,
      "learning_rate": 9.632426988922457e-05,
      "loss": 0.8821,
      "step": 2060
    },
    {
      "epoch": 1.5711048158640226,
      "grad_norm": 0.7108446359634399,
      "learning_rate": 9.531722054380666e-05,
      "loss": 0.8821,
      "step": 2080
    },
    {
      "epoch": 1.5862134088762985,
      "grad_norm": 0.7549437880516052,
      "learning_rate": 9.431017119838873e-05,
      "loss": 0.8824,
      "step": 2100
    },
    {
      "epoch": 1.601322001888574,
      "grad_norm": 0.8214563131332397,
      "learning_rate": 9.33031218529708e-05,
      "loss": 0.8829,
      "step": 2120
    },
    {
      "epoch": 1.6164305949008497,
      "grad_norm": 0.7277929782867432,
      "learning_rate": 9.229607250755287e-05,
      "loss": 0.8977,
      "step": 2140
    },
    {
      "epoch": 1.6315391879131256,
      "grad_norm": 0.7060758471488953,
      "learning_rate": 9.128902316213494e-05,
      "loss": 0.8767,
      "step": 2160
    },
    {
      "epoch": 1.6466477809254014,
      "grad_norm": 0.723488986492157,
      "learning_rate": 9.028197381671703e-05,
      "loss": 0.8825,
      "step": 2180
    },
    {
      "epoch": 1.661756373937677,
      "grad_norm": 0.7649081349372864,
      "learning_rate": 8.927492447129909e-05,
      "loss": 0.8899,
      "step": 2200
    },
    {
      "epoch": 1.6768649669499527,
      "grad_norm": 0.7516403198242188,
      "learning_rate": 8.826787512588117e-05,
      "loss": 0.874,
      "step": 2220
    },
    {
      "epoch": 1.6919735599622285,
      "grad_norm": 0.7289883494377136,
      "learning_rate": 8.726082578046326e-05,
      "loss": 0.8566,
      "step": 2240
    },
    {
      "epoch": 1.7070821529745044,
      "grad_norm": 0.6992005705833435,
      "learning_rate": 8.625377643504532e-05,
      "loss": 0.8759,
      "step": 2260
    },
    {
      "epoch": 1.72219074598678,
      "grad_norm": 0.7483837604522705,
      "learning_rate": 8.52467270896274e-05,
      "loss": 0.8724,
      "step": 2280
    },
    {
      "epoch": 1.7372993389990556,
      "grad_norm": 0.7625249624252319,
      "learning_rate": 8.423967774420947e-05,
      "loss": 0.8796,
      "step": 2300
    },
    {
      "epoch": 1.7524079320113315,
      "grad_norm": 0.7595409154891968,
      "learning_rate": 8.323262839879154e-05,
      "loss": 0.9149,
      "step": 2320
    },
    {
      "epoch": 1.7675165250236071,
      "grad_norm": 0.8370762467384338,
      "learning_rate": 8.222557905337363e-05,
      "loss": 0.8978,
      "step": 2340
    },
    {
      "epoch": 1.7826251180358827,
      "grad_norm": 0.715568482875824,
      "learning_rate": 8.121852970795569e-05,
      "loss": 0.8714,
      "step": 2360
    },
    {
      "epoch": 1.7977337110481586,
      "grad_norm": 0.7875036001205444,
      "learning_rate": 8.021148036253777e-05,
      "loss": 0.8815,
      "step": 2380
    },
    {
      "epoch": 1.8128423040604345,
      "grad_norm": 0.6773402094841003,
      "learning_rate": 7.920443101711985e-05,
      "loss": 0.8654,
      "step": 2400
    },
    {
      "epoch": 1.82795089707271,
      "grad_norm": 0.810448169708252,
      "learning_rate": 7.819738167170192e-05,
      "loss": 0.862,
      "step": 2420
    },
    {
      "epoch": 1.8430594900849857,
      "grad_norm": 0.7632348537445068,
      "learning_rate": 7.719033232628399e-05,
      "loss": 0.8578,
      "step": 2440
    },
    {
      "epoch": 1.8581680830972616,
      "grad_norm": 0.6808034777641296,
      "learning_rate": 7.618328298086606e-05,
      "loss": 0.8556,
      "step": 2460
    },
    {
      "epoch": 1.8732766761095374,
      "grad_norm": 0.8132623434066772,
      "learning_rate": 7.517623363544815e-05,
      "loss": 0.8627,
      "step": 2480
    },
    {
      "epoch": 1.888385269121813,
      "grad_norm": 0.6929353475570679,
      "learning_rate": 7.416918429003022e-05,
      "loss": 0.843,
      "step": 2500
    },
    {
      "epoch": 1.9034938621340887,
      "grad_norm": 0.7497186660766602,
      "learning_rate": 7.316213494461229e-05,
      "loss": 0.8704,
      "step": 2520
    },
    {
      "epoch": 1.9186024551463645,
      "grad_norm": 0.6687793731689453,
      "learning_rate": 7.215508559919436e-05,
      "loss": 0.839,
      "step": 2540
    },
    {
      "epoch": 1.9337110481586404,
      "grad_norm": 0.6806734204292297,
      "learning_rate": 7.114803625377643e-05,
      "loss": 0.8583,
      "step": 2560
    },
    {
      "epoch": 1.948819641170916,
      "grad_norm": 0.76473069190979,
      "learning_rate": 7.01409869083585e-05,
      "loss": 0.8682,
      "step": 2580
    },
    {
      "epoch": 1.9639282341831916,
      "grad_norm": 0.6730599403381348,
      "learning_rate": 6.913393756294059e-05,
      "loss": 0.8575,
      "step": 2600
    },
    {
      "epoch": 1.9790368271954675,
      "grad_norm": 0.7264896631240845,
      "learning_rate": 6.812688821752266e-05,
      "loss": 0.872,
      "step": 2620
    },
    {
      "epoch": 1.994145420207743,
      "grad_norm": 0.6782099008560181,
      "learning_rate": 6.711983887210473e-05,
      "loss": 0.8455,
      "step": 2640
    }
  ],
  "logging_steps": 20,
  "max_steps": 3972,
  "num_input_tokens_seen": 0,
  "num_train_epochs": 3,
  "save_steps": 500,
  "stateful_callbacks": {
    "TrainerControl": {
      "args": {
        "should_epoch_stop": false,
        "should_evaluate": false,
        "should_log": false,
        "should_save": true,
        "should_training_stop": false
      },
      "attributes": {}
    }
  },
  "total_flos": 9.323610643082772e+17,
  "train_batch_size": 4,
  "trial_name": null,
  "trial_params": null
}
