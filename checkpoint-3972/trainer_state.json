{
  "best_global_step": null,
  "best_metric": null,
  "best_model_checkpoint": null,
  "epoch": 3.0,
  "eval_steps": 500,
  "global_step": 3972,
  "is_hyper_param_search": false,
  "is_local_process_zero": true,
  "is_world_process_zero": true,
  "log_history": [
    {
      "epoch": 0.015108593012275733,
      "grad_norm": 0.638580322265625,
      "learning_rate": 0.000199043303121853,
      "loss": 2.1554,
      "step": 20
    },
    {
      "epoch": 0.030217186024551465,
      "grad_norm": 0.6124690175056458,
      "learning_rate": 0.00019803625377643507,
      "loss": 1.867,
      "step": 40
    },
    {
      "epoch": 0.0453257790368272,
      "grad_norm": 0.5913940668106079,
      "learning_rate": 0.00019702920443101713,
      "loss": 1.7297,
      "step": 60
    },
    {
      "epoch": 0.06043437204910293,
      "grad_norm": 0.7025613188743591,
      "learning_rate": 0.0001960221550855992,
      "loss": 1.7221,
      "step": 80
    },
    {
      "epoch": 0.07554296506137866,
      "grad_norm": 0.7115285992622375,
      "learning_rate": 0.00019501510574018128,
      "loss": 1.6675,
      "step": 100
    },
    {
      "epoch": 0.0906515580736544,
      "grad_norm": 0.6836743354797363,
      "learning_rate": 0.00019400805639476336,
      "loss": 1.6591,
      "step": 120
    },
    {
      "epoch": 0.10576015108593012,
      "grad_norm": 0.7489951252937317,
      "learning_rate": 0.00019300100704934542,
      "loss": 1.5474,
      "step": 140
    },
    {
      "epoch": 0.12086874409820586,
      "grad_norm": 0.6644171476364136,
      "learning_rate": 0.0001919939577039275,
      "loss": 1.5238,
      "step": 160
    },
    {
      "epoch": 0.1359773371104816,
      "grad_norm": 0.6642056703567505,
      "learning_rate": 0.0001909869083585096,
      "loss": 1.5068,
      "step": 180
    },
    {
      "epoch": 0.1510859301227573,
      "grad_norm": 0.6416130065917969,
      "learning_rate": 0.00018997985901309165,
      "loss": 1.4648,
      "step": 200
    },
    {
      "epoch": 0.16619452313503305,
      "grad_norm": 0.6407029032707214,
      "learning_rate": 0.00018897280966767373,
      "loss": 1.4511,
      "step": 220
    },
    {
      "epoch": 0.1813031161473088,
      "grad_norm": 0.6885688900947571,
      "learning_rate": 0.0001879657603222558,
      "loss": 1.4635,
      "step": 240
    },
    {
      "epoch": 0.1964117091595845,
      "grad_norm": 0.6579564213752747,
      "learning_rate": 0.00018695871097683788,
      "loss": 1.3833,
      "step": 260
    },
    {
      "epoch": 0.21152030217186024,
      "grad_norm": 0.6821775436401367,
      "learning_rate": 0.00018595166163141996,
      "loss": 1.3889,
      "step": 280
    },
    {
      "epoch": 0.22662889518413598,
      "grad_norm": 0.6145583987236023,
      "learning_rate": 0.00018494461228600202,
      "loss": 1.3926,
      "step": 300
    },
    {
      "epoch": 0.24173748819641172,
      "grad_norm": 0.6227560043334961,
      "learning_rate": 0.0001839375629405841,
      "loss": 1.3652,
      "step": 320
    },
    {
      "epoch": 0.25684608120868746,
      "grad_norm": 0.7036808133125305,
      "learning_rate": 0.00018293051359516616,
      "loss": 1.3351,
      "step": 340
    },
    {
      "epoch": 0.2719546742209632,
      "grad_norm": 0.6674836874008179,
      "learning_rate": 0.00018192346424974825,
      "loss": 1.2841,
      "step": 360
    },
    {
      "epoch": 0.2870632672332389,
      "grad_norm": 0.7363541722297668,
      "learning_rate": 0.0001809164149043303,
      "loss": 1.3102,
      "step": 380
    },
    {
      "epoch": 0.3021718602455146,
      "grad_norm": 0.6525669097900391,
      "learning_rate": 0.0001799093655589124,
      "loss": 1.2601,
      "step": 400
    },
    {
      "epoch": 0.31728045325779036,
      "grad_norm": 0.6826169490814209,
      "learning_rate": 0.00017890231621349448,
      "loss": 1.2788,
      "step": 420
    },
    {
      "epoch": 0.3323890462700661,
      "grad_norm": 0.6846610307693481,
      "learning_rate": 0.00017789526686807654,
      "loss": 1.2413,
      "step": 440
    },
    {
      "epoch": 0.34749763928234184,
      "grad_norm": 0.737987756729126,
      "learning_rate": 0.00017688821752265862,
      "loss": 1.2721,
      "step": 460
    },
    {
      "epoch": 0.3626062322946176,
      "grad_norm": 0.6671044826507568,
      "learning_rate": 0.0001758811681772407,
      "loss": 1.2122,
      "step": 480
    },
    {
      "epoch": 0.3777148253068933,
      "grad_norm": 0.62265545129776,
      "learning_rate": 0.00017487411883182277,
      "loss": 1.2113,
      "step": 500
    },
    {
      "epoch": 0.392823418319169,
      "grad_norm": 0.7344897389411926,
      "learning_rate": 0.00017386706948640482,
      "loss": 1.1999,
      "step": 520
    },
    {
      "epoch": 0.40793201133144474,
      "grad_norm": 0.6948499083518982,
      "learning_rate": 0.0001728600201409869,
      "loss": 1.2143,
      "step": 540
    },
    {
      "epoch": 0.4230406043437205,
      "grad_norm": 0.5802909135818481,
      "learning_rate": 0.000171852970795569,
      "loss": 1.1561,
      "step": 560
    },
    {
      "epoch": 0.4381491973559962,
      "grad_norm": 0.6689253449440002,
      "learning_rate": 0.00017084592145015105,
      "loss": 1.1617,
      "step": 580
    },
    {
      "epoch": 0.45325779036827196,
      "grad_norm": 0.666337788105011,
      "learning_rate": 0.00016983887210473314,
      "loss": 1.1379,
      "step": 600
    },
    {
      "epoch": 0.4683663833805477,
      "grad_norm": 0.708098828792572,
      "learning_rate": 0.00016883182275931522,
      "loss": 1.1678,
      "step": 620
    },
    {
      "epoch": 0.48347497639282344,
      "grad_norm": 0.6483791470527649,
      "learning_rate": 0.00016782477341389728,
      "loss": 1.1626,
      "step": 640
    },
    {
      "epoch": 0.4985835694050991,
      "grad_norm": 0.7273135185241699,
      "learning_rate": 0.00016681772406847934,
      "loss": 1.1553,
      "step": 660
    },
    {
      "epoch": 0.5136921624173749,
      "grad_norm": 0.7223193049430847,
      "learning_rate": 0.00016581067472306145,
      "loss": 1.1786,
      "step": 680
    },
    {
      "epoch": 0.5288007554296507,
      "grad_norm": 0.8509509563446045,
      "learning_rate": 0.0001648036253776435,
      "loss": 1.1515,
      "step": 700
    },
    {
      "epoch": 0.5439093484419264,
      "grad_norm": 0.6471524238586426,
      "learning_rate": 0.00016379657603222557,
      "loss": 1.1366,
      "step": 720
    },
    {
      "epoch": 0.559017941454202,
      "grad_norm": 0.6572981476783752,
      "learning_rate": 0.00016278952668680768,
      "loss": 1.1219,
      "step": 740
    },
    {
      "epoch": 0.5741265344664778,
      "grad_norm": 0.6402274370193481,
      "learning_rate": 0.00016178247734138974,
      "loss": 1.1575,
      "step": 760
    },
    {
      "epoch": 0.5892351274787535,
      "grad_norm": 0.6382126212120056,
      "learning_rate": 0.0001607754279959718,
      "loss": 1.1673,
      "step": 780
    },
    {
      "epoch": 0.6043437204910292,
      "grad_norm": 0.6923805475234985,
      "learning_rate": 0.00015976837865055388,
      "loss": 1.1344,
      "step": 800
    },
    {
      "epoch": 0.619452313503305,
      "grad_norm": 0.6478564143180847,
      "learning_rate": 0.00015876132930513597,
      "loss": 1.1009,
      "step": 820
    },
    {
      "epoch": 0.6345609065155807,
      "grad_norm": 0.6106889247894287,
      "learning_rate": 0.00015775427995971803,
      "loss": 1.1523,
      "step": 840
    },
    {
      "epoch": 0.6496694995278565,
      "grad_norm": 0.6930863857269287,
      "learning_rate": 0.0001567472306143001,
      "loss": 1.1116,
      "step": 860
    },
    {
      "epoch": 0.6647780925401322,
      "grad_norm": 0.6119394898414612,
      "learning_rate": 0.0001557401812688822,
      "loss": 1.1025,
      "step": 880
    },
    {
      "epoch": 0.6798866855524079,
      "grad_norm": 0.7265839576721191,
      "learning_rate": 0.00015473313192346426,
      "loss": 1.1264,
      "step": 900
    },
    {
      "epoch": 0.6949952785646837,
      "grad_norm": 0.7039998173713684,
      "learning_rate": 0.00015372608257804631,
      "loss": 1.099,
      "step": 920
    },
    {
      "epoch": 0.7101038715769594,
      "grad_norm": 0.7395510077476501,
      "learning_rate": 0.00015271903323262843,
      "loss": 1.0824,
      "step": 940
    },
    {
      "epoch": 0.7252124645892352,
      "grad_norm": 0.6820393204689026,
      "learning_rate": 0.00015171198388721049,
      "loss": 1.0608,
      "step": 960
    },
    {
      "epoch": 0.7403210576015109,
      "grad_norm": 0.7570927143096924,
      "learning_rate": 0.00015070493454179254,
      "loss": 1.1041,
      "step": 980
    },
    {
      "epoch": 0.7554296506137866,
      "grad_norm": 0.5917067527770996,
      "learning_rate": 0.00014969788519637463,
      "loss": 1.1095,
      "step": 1000
    },
    {
      "epoch": 0.7705382436260623,
      "grad_norm": 0.7482699751853943,
      "learning_rate": 0.00014869083585095671,
      "loss": 1.1072,
      "step": 1020
    },
    {
      "epoch": 0.785646836638338,
      "grad_norm": 0.8510087132453918,
      "learning_rate": 0.00014768378650553877,
      "loss": 1.0509,
      "step": 1040
    },
    {
      "epoch": 0.8007554296506137,
      "grad_norm": 0.6339656710624695,
      "learning_rate": 0.00014667673716012086,
      "loss": 1.0813,
      "step": 1060
    },
    {
      "epoch": 0.8158640226628895,
      "grad_norm": 0.7859771847724915,
      "learning_rate": 0.00014566968781470294,
      "loss": 1.0397,
      "step": 1080
    },
    {
      "epoch": 0.8309726156751652,
      "grad_norm": 0.6934078931808472,
      "learning_rate": 0.000144662638469285,
      "loss": 1.0283,
      "step": 1100
    },
    {
      "epoch": 0.846081208687441,
      "grad_norm": 0.8056946396827698,
      "learning_rate": 0.0001436555891238671,
      "loss": 1.0491,
      "step": 1120
    },
    {
      "epoch": 0.8611898016997167,
      "grad_norm": 0.6803275942802429,
      "learning_rate": 0.00014264853977844914,
      "loss": 1.068,
      "step": 1140
    },
    {
      "epoch": 0.8762983947119924,
      "grad_norm": 0.6918123364448547,
      "learning_rate": 0.00014164149043303123,
      "loss": 1.078,
      "step": 1160
    },
    {
      "epoch": 0.8914069877242682,
      "grad_norm": 0.7286608219146729,
      "learning_rate": 0.0001406344410876133,
      "loss": 1.0838,
      "step": 1180
    },
    {
      "epoch": 0.9065155807365439,
      "grad_norm": 0.7803124785423279,
      "learning_rate": 0.00013962739174219537,
      "loss": 1.0333,
      "step": 1200
    },
    {
      "epoch": 0.9216241737488197,
      "grad_norm": 0.7093648314476013,
      "learning_rate": 0.00013862034239677746,
      "loss": 1.0212,
      "step": 1220
    },
    {
      "epoch": 0.9367327667610954,
      "grad_norm": 0.7172715067863464,
      "learning_rate": 0.00013761329305135952,
      "loss": 1.0225,
      "step": 1240
    },
    {
      "epoch": 0.9518413597733711,
      "grad_norm": 0.8255679607391357,
      "learning_rate": 0.0001366062437059416,
      "loss": 1.0294,
      "step": 1260
    },
    {
      "epoch": 0.9669499527856469,
      "grad_norm": 0.6921229362487793,
      "learning_rate": 0.00013559919436052366,
      "loss": 1.0184,
      "step": 1280
    },
    {
      "epoch": 0.9820585457979226,
      "grad_norm": 0.7010710835456848,
      "learning_rate": 0.00013459214501510575,
      "loss": 1.0473,
      "step": 1300
    },
    {
      "epoch": 0.9971671388101983,
      "grad_norm": 0.6477928161621094,
      "learning_rate": 0.00013358509566968783,
      "loss": 1.0388,
      "step": 1320
    },
    {
      "epoch": 1.0120868744098206,
      "grad_norm": 0.6787996888160706,
      "learning_rate": 0.0001325780463242699,
      "loss": 0.958,
      "step": 1340
    },
    {
      "epoch": 1.0271954674220962,
      "grad_norm": 0.6528811454772949,
      "learning_rate": 0.00013157099697885198,
      "loss": 0.9142,
      "step": 1360
    },
    {
      "epoch": 1.042304060434372,
      "grad_norm": 0.7415065765380859,
      "learning_rate": 0.00013056394763343403,
      "loss": 0.9262,
      "step": 1380
    },
    {
      "epoch": 1.0574126534466477,
      "grad_norm": 0.7211471199989319,
      "learning_rate": 0.00012955689828801612,
      "loss": 0.955,
      "step": 1400
    },
    {
      "epoch": 1.0725212464589235,
      "grad_norm": 0.7308480739593506,
      "learning_rate": 0.00012854984894259818,
      "loss": 0.9251,
      "step": 1420
    },
    {
      "epoch": 1.0876298394711992,
      "grad_norm": 0.7025598883628845,
      "learning_rate": 0.00012754279959718026,
      "loss": 0.9242,
      "step": 1440
    },
    {
      "epoch": 1.102738432483475,
      "grad_norm": 0.6775097846984863,
      "learning_rate": 0.00012653575025176235,
      "loss": 0.9489,
      "step": 1460
    },
    {
      "epoch": 1.1178470254957507,
      "grad_norm": 0.652665376663208,
      "learning_rate": 0.0001255287009063444,
      "loss": 0.8836,
      "step": 1480
    },
    {
      "epoch": 1.1329556185080265,
      "grad_norm": 0.6998504400253296,
      "learning_rate": 0.0001245216515609265,
      "loss": 0.9681,
      "step": 1500
    },
    {
      "epoch": 1.1480642115203021,
      "grad_norm": 0.6345914006233215,
      "learning_rate": 0.00012351460221550858,
      "loss": 0.9424,
      "step": 1520
    },
    {
      "epoch": 1.163172804532578,
      "grad_norm": 0.6555542945861816,
      "learning_rate": 0.00012250755287009064,
      "loss": 0.8946,
      "step": 1540
    },
    {
      "epoch": 1.1782813975448536,
      "grad_norm": 0.7820575833320618,
      "learning_rate": 0.00012150050352467271,
      "loss": 0.909,
      "step": 1560
    },
    {
      "epoch": 1.1933899905571295,
      "grad_norm": 0.6515637636184692,
      "learning_rate": 0.00012049345417925479,
      "loss": 0.9255,
      "step": 1580
    },
    {
      "epoch": 1.208498583569405,
      "grad_norm": 0.6812759637832642,
      "learning_rate": 0.00011948640483383686,
      "loss": 0.8935,
      "step": 1600
    },
    {
      "epoch": 1.2236071765816807,
      "grad_norm": 0.6611970067024231,
      "learning_rate": 0.00011847935548841894,
      "loss": 0.8981,
      "step": 1620
    },
    {
      "epoch": 1.2387157695939566,
      "grad_norm": 0.7894744277000427,
      "learning_rate": 0.00011747230614300102,
      "loss": 0.8934,
      "step": 1640
    },
    {
      "epoch": 1.2538243626062324,
      "grad_norm": 0.7084535360336304,
      "learning_rate": 0.0001164652567975831,
      "loss": 0.8972,
      "step": 1660
    },
    {
      "epoch": 1.268932955618508,
      "grad_norm": 0.6829096674919128,
      "learning_rate": 0.00011545820745216515,
      "loss": 0.9353,
      "step": 1680
    },
    {
      "epoch": 1.2840415486307837,
      "grad_norm": 0.6458596587181091,
      "learning_rate": 0.00011445115810674725,
      "loss": 0.8825,
      "step": 1700
    },
    {
      "epoch": 1.2991501416430595,
      "grad_norm": 0.716256856918335,
      "learning_rate": 0.00011344410876132931,
      "loss": 0.9416,
      "step": 1720
    },
    {
      "epoch": 1.3142587346553352,
      "grad_norm": 0.6466896533966064,
      "learning_rate": 0.00011243705941591138,
      "loss": 0.9052,
      "step": 1740
    },
    {
      "epoch": 1.329367327667611,
      "grad_norm": 0.7117520570755005,
      "learning_rate": 0.00011143001007049345,
      "loss": 0.9238,
      "step": 1760
    },
    {
      "epoch": 1.3444759206798866,
      "grad_norm": 0.6773107647895813,
      "learning_rate": 0.00011042296072507554,
      "loss": 0.8678,
      "step": 1780
    },
    {
      "epoch": 1.3595845136921625,
      "grad_norm": 0.7109878659248352,
      "learning_rate": 0.00010941591137965761,
      "loss": 0.9376,
      "step": 1800
    },
    {
      "epoch": 1.3746931067044381,
      "grad_norm": 0.6877878904342651,
      "learning_rate": 0.00010840886203423968,
      "loss": 0.8701,
      "step": 1820
    },
    {
      "epoch": 1.3898016997167137,
      "grad_norm": 0.7716924548149109,
      "learning_rate": 0.00010740181268882177,
      "loss": 0.8978,
      "step": 1840
    },
    {
      "epoch": 1.4049102927289896,
      "grad_norm": 0.7168180346488953,
      "learning_rate": 0.00010639476334340384,
      "loss": 0.8938,
      "step": 1860
    },
    {
      "epoch": 1.4200188857412654,
      "grad_norm": 0.7712883353233337,
      "learning_rate": 0.00010538771399798591,
      "loss": 0.9121,
      "step": 1880
    },
    {
      "epoch": 1.435127478753541,
      "grad_norm": 0.742200493812561,
      "learning_rate": 0.00010438066465256797,
      "loss": 0.9196,
      "step": 1900
    },
    {
      "epoch": 1.4502360717658167,
      "grad_norm": 0.6934712529182434,
      "learning_rate": 0.00010337361530715007,
      "loss": 0.9169,
      "step": 1920
    },
    {
      "epoch": 1.4653446647780926,
      "grad_norm": 0.7364057302474976,
      "learning_rate": 0.00010236656596173213,
      "loss": 0.8874,
      "step": 1940
    },
    {
      "epoch": 1.4804532577903684,
      "grad_norm": 0.7111160755157471,
      "learning_rate": 0.0001013595166163142,
      "loss": 0.8839,
      "step": 1960
    },
    {
      "epoch": 1.495561850802644,
      "grad_norm": 0.7938785552978516,
      "learning_rate": 0.00010035246727089628,
      "loss": 0.9016,
      "step": 1980
    },
    {
      "epoch": 1.5106704438149197,
      "grad_norm": 0.7610501050949097,
      "learning_rate": 9.934541792547835e-05,
      "loss": 0.8903,
      "step": 2000
    },
    {
      "epoch": 1.5257790368271955,
      "grad_norm": 0.6649030447006226,
      "learning_rate": 9.833836858006043e-05,
      "loss": 0.9041,
      "step": 2020
    },
    {
      "epoch": 1.5408876298394714,
      "grad_norm": 0.6983354687690735,
      "learning_rate": 9.73313192346425e-05,
      "loss": 0.8738,
      "step": 2040
    },
    {
      "epoch": 1.5559962228517468,
      "grad_norm": 0.7853218913078308,
      "learning_rate": 9.632426988922457e-05,
      "loss": 0.8821,
      "step": 2060
    },
    {
      "epoch": 1.5711048158640226,
      "grad_norm": 0.7108446359634399,
      "learning_rate": 9.531722054380666e-05,
      "loss": 0.8821,
      "step": 2080
    },
    {
      "epoch": 1.5862134088762985,
      "grad_norm": 0.7549437880516052,
      "learning_rate": 9.431017119838873e-05,
      "loss": 0.8824,
      "step": 2100
    },
    {
      "epoch": 1.601322001888574,
      "grad_norm": 0.8214563131332397,
      "learning_rate": 9.33031218529708e-05,
      "loss": 0.8829,
      "step": 2120
    },
    {
      "epoch": 1.6164305949008497,
      "grad_norm": 0.7277929782867432,
      "learning_rate": 9.229607250755287e-05,
      "loss": 0.8977,
      "step": 2140
    },
    {
      "epoch": 1.6315391879131256,
      "grad_norm": 0.7060758471488953,
      "learning_rate": 9.128902316213494e-05,
      "loss": 0.8767,
      "step": 2160
    },
    {
      "epoch": 1.6466477809254014,
      "grad_norm": 0.723488986492157,
      "learning_rate": 9.028197381671703e-05,
      "loss": 0.8825,
      "step": 2180
    },
    {
      "epoch": 1.661756373937677,
      "grad_norm": 0.7649081349372864,
      "learning_rate": 8.927492447129909e-05,
      "loss": 0.8899,
      "step": 2200
    },
    {
      "epoch": 1.6768649669499527,
      "grad_norm": 0.7516403198242188,
      "learning_rate": 8.826787512588117e-05,
      "loss": 0.874,
      "step": 2220
    },
    {
      "epoch": 1.6919735599622285,
      "grad_norm": 0.7289883494377136,
      "learning_rate": 8.726082578046326e-05,
      "loss": 0.8566,
      "step": 2240
    },
    {
      "epoch": 1.7070821529745044,
      "grad_norm": 0.6992005705833435,
      "learning_rate": 8.625377643504532e-05,
      "loss": 0.8759,
      "step": 2260
    },
    {
      "epoch": 1.72219074598678,
      "grad_norm": 0.7483837604522705,
      "learning_rate": 8.52467270896274e-05,
      "loss": 0.8724,
      "step": 2280
    },
    {
      "epoch": 1.7372993389990556,
      "grad_norm": 0.7625249624252319,
      "learning_rate": 8.423967774420947e-05,
      "loss": 0.8796,
      "step": 2300
    },
    {
      "epoch": 1.7524079320113315,
      "grad_norm": 0.7595409154891968,
      "learning_rate": 8.323262839879154e-05,
      "loss": 0.9149,
      "step": 2320
    },
    {
      "epoch": 1.7675165250236071,
      "grad_norm": 0.8370762467384338,
      "learning_rate": 8.222557905337363e-05,
      "loss": 0.8978,
      "step": 2340
    },
    {
      "epoch": 1.7826251180358827,
      "grad_norm": 0.715568482875824,
      "learning_rate": 8.121852970795569e-05,
      "loss": 0.8714,
      "step": 2360
    },
    {
      "epoch": 1.7977337110481586,
      "grad_norm": 0.7875036001205444,
      "learning_rate": 8.021148036253777e-05,
      "loss": 0.8815,
      "step": 2380
    },
    {
      "epoch": 1.8128423040604345,
      "grad_norm": 0.6773402094841003,
      "learning_rate": 7.920443101711985e-05,
      "loss": 0.8654,
      "step": 2400
    },
    {
      "epoch": 1.82795089707271,
      "grad_norm": 0.810448169708252,
      "learning_rate": 7.819738167170192e-05,
      "loss": 0.862,
      "step": 2420
    },
    {
      "epoch": 1.8430594900849857,
      "grad_norm": 0.7632348537445068,
      "learning_rate": 7.719033232628399e-05,
      "loss": 0.8578,
      "step": 2440
    },
    {
      "epoch": 1.8581680830972616,
      "grad_norm": 0.6808034777641296,
      "learning_rate": 7.618328298086606e-05,
      "loss": 0.8556,
      "step": 2460
    },
    {
      "epoch": 1.8732766761095374,
      "grad_norm": 0.8132623434066772,
      "learning_rate": 7.517623363544815e-05,
      "loss": 0.8627,
      "step": 2480
    },
    {
      "epoch": 1.888385269121813,
      "grad_norm": 0.6929353475570679,
      "learning_rate": 7.416918429003022e-05,
      "loss": 0.843,
      "step": 2500
    },
    {
      "epoch": 1.9034938621340887,
      "grad_norm": 0.7497186660766602,
      "learning_rate": 7.316213494461229e-05,
      "loss": 0.8704,
      "step": 2520
    },
    {
      "epoch": 1.9186024551463645,
      "grad_norm": 0.6687793731689453,
      "learning_rate": 7.215508559919436e-05,
      "loss": 0.839,
      "step": 2540
    },
    {
      "epoch": 1.9337110481586404,
      "grad_norm": 0.6806734204292297,
      "learning_rate": 7.114803625377643e-05,
      "loss": 0.8583,
      "step": 2560
    },
    {
      "epoch": 1.948819641170916,
      "grad_norm": 0.76473069190979,
      "learning_rate": 7.01409869083585e-05,
      "loss": 0.8682,
      "step": 2580
    },
    {
      "epoch": 1.9639282341831916,
      "grad_norm": 0.6730599403381348,
      "learning_rate": 6.913393756294059e-05,
      "loss": 0.8575,
      "step": 2600
    },
    {
      "epoch": 1.9790368271954675,
      "grad_norm": 0.7264896631240845,
      "learning_rate": 6.812688821752266e-05,
      "loss": 0.872,
      "step": 2620
    },
    {
      "epoch": 1.994145420207743,
      "grad_norm": 0.6782099008560181,
      "learning_rate": 6.711983887210473e-05,
      "loss": 0.8455,
      "step": 2640
    },
    {
      "epoch": 2.0090651558073653,
      "grad_norm": 0.6775258183479309,
      "learning_rate": 6.611278952668682e-05,
      "loss": 0.806,
      "step": 2660
    },
    {
      "epoch": 2.024173748819641,
      "grad_norm": 0.6463153958320618,
      "learning_rate": 6.510574018126888e-05,
      "loss": 0.7499,
      "step": 2680
    },
    {
      "epoch": 2.039282341831917,
      "grad_norm": 0.6955340504646301,
      "learning_rate": 6.409869083585096e-05,
      "loss": 0.7612,
      "step": 2700
    },
    {
      "epoch": 2.0543909348441924,
      "grad_norm": 0.7564371228218079,
      "learning_rate": 6.309164149043303e-05,
      "loss": 0.7378,
      "step": 2720
    },
    {
      "epoch": 2.0694995278564683,
      "grad_norm": 0.750594973564148,
      "learning_rate": 6.20845921450151e-05,
      "loss": 0.7586,
      "step": 2740
    },
    {
      "epoch": 2.084608120868744,
      "grad_norm": 0.6517391204833984,
      "learning_rate": 6.107754279959719e-05,
      "loss": 0.7216,
      "step": 2760
    },
    {
      "epoch": 2.09971671388102,
      "grad_norm": 0.6914400458335876,
      "learning_rate": 6.007049345417926e-05,
      "loss": 0.8072,
      "step": 2780
    },
    {
      "epoch": 2.1148253068932954,
      "grad_norm": 0.7156044244766235,
      "learning_rate": 5.9063444108761336e-05,
      "loss": 0.7524,
      "step": 2800
    },
    {
      "epoch": 2.1299338999055712,
      "grad_norm": 0.7366833090782166,
      "learning_rate": 5.80563947633434e-05,
      "loss": 0.7804,
      "step": 2820
    },
    {
      "epoch": 2.145042492917847,
      "grad_norm": 0.6558067798614502,
      "learning_rate": 5.704934541792548e-05,
      "loss": 0.7408,
      "step": 2840
    },
    {
      "epoch": 2.160151085930123,
      "grad_norm": 0.7388297319412231,
      "learning_rate": 5.604229607250756e-05,
      "loss": 0.745,
      "step": 2860
    },
    {
      "epoch": 2.1752596789423984,
      "grad_norm": 0.76006680727005,
      "learning_rate": 5.503524672708963e-05,
      "loss": 0.7362,
      "step": 2880
    },
    {
      "epoch": 2.190368271954674,
      "grad_norm": 0.7554231882095337,
      "learning_rate": 5.402819738167171e-05,
      "loss": 0.7343,
      "step": 2900
    },
    {
      "epoch": 2.20547686496695,
      "grad_norm": 0.750811755657196,
      "learning_rate": 5.3021148036253773e-05,
      "loss": 0.7635,
      "step": 2920
    },
    {
      "epoch": 2.220585457979226,
      "grad_norm": 0.7683538198471069,
      "learning_rate": 5.201409869083585e-05,
      "loss": 0.7645,
      "step": 2940
    },
    {
      "epoch": 2.2356940509915013,
      "grad_norm": 0.7954263687133789,
      "learning_rate": 5.1007049345417924e-05,
      "loss": 0.7331,
      "step": 2960
    },
    {
      "epoch": 2.250802644003777,
      "grad_norm": 0.8725354671478271,
      "learning_rate": 5e-05,
      "loss": 0.7489,
      "step": 2980
    },
    {
      "epoch": 2.265911237016053,
      "grad_norm": 0.7743440866470337,
      "learning_rate": 4.8992950654582074e-05,
      "loss": 0.7491,
      "step": 3000
    },
    {
      "epoch": 2.2810198300283284,
      "grad_norm": 0.8773398995399475,
      "learning_rate": 4.798590130916415e-05,
      "loss": 0.7595,
      "step": 3020
    },
    {
      "epoch": 2.2961284230406043,
      "grad_norm": 0.7695181369781494,
      "learning_rate": 4.6978851963746225e-05,
      "loss": 0.7414,
      "step": 3040
    },
    {
      "epoch": 2.31123701605288,
      "grad_norm": 0.7810695171356201,
      "learning_rate": 4.59718026183283e-05,
      "loss": 0.7711,
      "step": 3060
    },
    {
      "epoch": 2.326345609065156,
      "grad_norm": 0.7310814261436462,
      "learning_rate": 4.4964753272910375e-05,
      "loss": 0.7583,
      "step": 3080
    },
    {
      "epoch": 2.3414542020774314,
      "grad_norm": 0.8249779939651489,
      "learning_rate": 4.395770392749245e-05,
      "loss": 0.7499,
      "step": 3100
    },
    {
      "epoch": 2.3565627950897072,
      "grad_norm": 0.7354727983474731,
      "learning_rate": 4.2950654582074525e-05,
      "loss": 0.7458,
      "step": 3120
    },
    {
      "epoch": 2.371671388101983,
      "grad_norm": 0.7690263986587524,
      "learning_rate": 4.19436052366566e-05,
      "loss": 0.7174,
      "step": 3140
    },
    {
      "epoch": 2.386779981114259,
      "grad_norm": 0.8204306960105896,
      "learning_rate": 4.093655589123867e-05,
      "loss": 0.7204,
      "step": 3160
    },
    {
      "epoch": 2.4018885741265343,
      "grad_norm": 0.7196217775344849,
      "learning_rate": 3.992950654582075e-05,
      "loss": 0.7682,
      "step": 3180
    },
    {
      "epoch": 2.41699716713881,
      "grad_norm": 0.728771448135376,
      "learning_rate": 3.8922457200402826e-05,
      "loss": 0.7626,
      "step": 3200
    },
    {
      "epoch": 2.432105760151086,
      "grad_norm": 0.8539761304855347,
      "learning_rate": 3.79154078549849e-05,
      "loss": 0.7447,
      "step": 3220
    },
    {
      "epoch": 2.4472143531633614,
      "grad_norm": 0.7283695340156555,
      "learning_rate": 3.690835850956697e-05,
      "loss": 0.7455,
      "step": 3240
    },
    {
      "epoch": 2.4623229461756373,
      "grad_norm": 0.8519406914710999,
      "learning_rate": 3.590130916414904e-05,
      "loss": 0.7489,
      "step": 3260
    },
    {
      "epoch": 2.477431539187913,
      "grad_norm": 0.7577463984489441,
      "learning_rate": 3.489425981873112e-05,
      "loss": 0.7332,
      "step": 3280
    },
    {
      "epoch": 2.492540132200189,
      "grad_norm": 0.7522422671318054,
      "learning_rate": 3.388721047331319e-05,
      "loss": 0.7484,
      "step": 3300
    },
    {
      "epoch": 2.507648725212465,
      "grad_norm": 0.8015846014022827,
      "learning_rate": 3.288016112789527e-05,
      "loss": 0.7391,
      "step": 3320
    },
    {
      "epoch": 2.5227573182247403,
      "grad_norm": 0.7665454745292664,
      "learning_rate": 3.187311178247734e-05,
      "loss": 0.7408,
      "step": 3340
    },
    {
      "epoch": 2.537865911237016,
      "grad_norm": 0.6900220513343811,
      "learning_rate": 3.0866062437059414e-05,
      "loss": 0.7267,
      "step": 3360
    },
    {
      "epoch": 2.552974504249292,
      "grad_norm": 0.7627242803573608,
      "learning_rate": 2.9859013091641493e-05,
      "loss": 0.6963,
      "step": 3380
    },
    {
      "epoch": 2.5680830972615674,
      "grad_norm": 0.6937912702560425,
      "learning_rate": 2.8851963746223565e-05,
      "loss": 0.7003,
      "step": 3400
    },
    {
      "epoch": 2.583191690273843,
      "grad_norm": 0.8397653698921204,
      "learning_rate": 2.784491440080564e-05,
      "loss": 0.7275,
      "step": 3420
    },
    {
      "epoch": 2.598300283286119,
      "grad_norm": 0.7843543887138367,
      "learning_rate": 2.683786505538772e-05,
      "loss": 0.7213,
      "step": 3440
    },
    {
      "epoch": 2.6134088762983945,
      "grad_norm": 0.7801468968391418,
      "learning_rate": 2.583081570996979e-05,
      "loss": 0.733,
      "step": 3460
    },
    {
      "epoch": 2.6285174693106703,
      "grad_norm": 0.9001993536949158,
      "learning_rate": 2.4823766364551865e-05,
      "loss": 0.7315,
      "step": 3480
    },
    {
      "epoch": 2.643626062322946,
      "grad_norm": 0.7869934439659119,
      "learning_rate": 2.3816717019133937e-05,
      "loss": 0.7272,
      "step": 3500
    },
    {
      "epoch": 2.658734655335222,
      "grad_norm": 0.7789368629455566,
      "learning_rate": 2.2809667673716012e-05,
      "loss": 0.7601,
      "step": 3520
    },
    {
      "epoch": 2.673843248347498,
      "grad_norm": 0.7572530508041382,
      "learning_rate": 2.1802618328298088e-05,
      "loss": 0.7251,
      "step": 3540
    },
    {
      "epoch": 2.6889518413597733,
      "grad_norm": 0.7620276212692261,
      "learning_rate": 2.0795568982880163e-05,
      "loss": 0.7545,
      "step": 3560
    },
    {
      "epoch": 2.704060434372049,
      "grad_norm": 0.7603978514671326,
      "learning_rate": 1.9788519637462235e-05,
      "loss": 0.7234,
      "step": 3580
    },
    {
      "epoch": 2.719169027384325,
      "grad_norm": 0.7570294737815857,
      "learning_rate": 1.8781470292044313e-05,
      "loss": 0.7249,
      "step": 3600
    },
    {
      "epoch": 2.7342776203966004,
      "grad_norm": 0.7163180112838745,
      "learning_rate": 1.7774420946626385e-05,
      "loss": 0.7295,
      "step": 3620
    },
    {
      "epoch": 2.7493862134088762,
      "grad_norm": 0.7795778512954712,
      "learning_rate": 1.676737160120846e-05,
      "loss": 0.7384,
      "step": 3640
    },
    {
      "epoch": 2.764494806421152,
      "grad_norm": 0.7650654315948486,
      "learning_rate": 1.5760322255790535e-05,
      "loss": 0.7151,
      "step": 3660
    },
    {
      "epoch": 2.7796033994334275,
      "grad_norm": 0.7396097779273987,
      "learning_rate": 1.4753272910372609e-05,
      "loss": 0.7508,
      "step": 3680
    },
    {
      "epoch": 2.7947119924457033,
      "grad_norm": 0.7104594707489014,
      "learning_rate": 1.3746223564954682e-05,
      "loss": 0.7262,
      "step": 3700
    },
    {
      "epoch": 2.809820585457979,
      "grad_norm": 0.8021177649497986,
      "learning_rate": 1.273917421953676e-05,
      "loss": 0.7466,
      "step": 3720
    },
    {
      "epoch": 2.824929178470255,
      "grad_norm": 0.6953269839286804,
      "learning_rate": 1.1732124874118833e-05,
      "loss": 0.7281,
      "step": 3740
    },
    {
      "epoch": 2.840037771482531,
      "grad_norm": 0.7733491063117981,
      "learning_rate": 1.0725075528700906e-05,
      "loss": 0.7255,
      "step": 3760
    },
    {
      "epoch": 2.8551463644948063,
      "grad_norm": 0.7736575603485107,
      "learning_rate": 9.718026183282982e-06,
      "loss": 0.7503,
      "step": 3780
    },
    {
      "epoch": 2.870254957507082,
      "grad_norm": 0.8047302961349487,
      "learning_rate": 8.710976837865057e-06,
      "loss": 0.7214,
      "step": 3800
    },
    {
      "epoch": 2.885363550519358,
      "grad_norm": 0.7734549045562744,
      "learning_rate": 7.70392749244713e-06,
      "loss": 0.7144,
      "step": 3820
    },
    {
      "epoch": 2.9004721435316334,
      "grad_norm": 0.7978493571281433,
      "learning_rate": 6.696878147029205e-06,
      "loss": 0.7148,
      "step": 3840
    },
    {
      "epoch": 2.9155807365439093,
      "grad_norm": 0.7501046061515808,
      "learning_rate": 5.689828801611279e-06,
      "loss": 0.7323,
      "step": 3860
    },
    {
      "epoch": 2.930689329556185,
      "grad_norm": 0.8202930092811584,
      "learning_rate": 4.682779456193353e-06,
      "loss": 0.7524,
      "step": 3880
    },
    {
      "epoch": 2.945797922568461,
      "grad_norm": 0.7556443810462952,
      "learning_rate": 3.6757301107754277e-06,
      "loss": 0.7296,
      "step": 3900
    },
    {
      "epoch": 2.960906515580737,
      "grad_norm": 0.6831679940223694,
      "learning_rate": 2.6686807653575025e-06,
      "loss": 0.73,
      "step": 3920
    },
    {
      "epoch": 2.976015108593012,
      "grad_norm": 0.7859598994255066,
      "learning_rate": 1.661631419939577e-06,
      "loss": 0.7309,
      "step": 3940
    },
    {
      "epoch": 2.991123701605288,
      "grad_norm": 0.7647049427032471,
      "learning_rate": 6.545820745216516e-07,
      "loss": 0.7071,
      "step": 3960
    }
  ],
  "logging_steps": 20,
  "max_steps": 3972,
  "num_input_tokens_seen": 0,
  "num_train_epochs": 3,
  "save_steps": 500,
  "stateful_callbacks": {
    "TrainerControl": {
      "args": {
        "should_epoch_stop": false,
        "should_evaluate": false,
        "should_log": false,
        "should_save": true,
        "should_training_stop": true
      },
      "attributes": {}
    }
  },
  "total_flos": 1.398541596462416e+18,
  "train_batch_size": 4,
  "trial_name": null,
  "trial_params": null
}
